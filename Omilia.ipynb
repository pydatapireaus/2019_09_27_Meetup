{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyData Piraeus - Sep, 2019\n",
    "\n",
    "## NLU Data Science\n",
    "\n",
    "\n",
    "### Language\n",
    "\n",
    "- Python 3\n",
    "\n",
    "### Tools\n",
    "\n",
    "- pandas\n",
    "- nltk\n",
    "- gensim\n",
    "- scikit-learn\n",
    "- pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from typing import List\n",
    "\n",
    "from utils.deep_learning import train, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# inputs\n",
    "fname_snips_train = './data/snips_train.csv'\n",
    "fname_snips_test = './data/snips_test.csv'\n",
    "fname_quora_train = './data/quora_train.csv'\n",
    "fname_quora_test = './data/quora_test.csv'\n",
    "fname_embeddings = '/workspace/deepNLU/mLearningNLU/embeddings/pretrained/GloVe/glove.840B.300d.bin'\n",
    "cache_dir = '/workspace/deepNLU/mLearningNLU/models/pytorch-transformers'\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading static embeddings: /workspace/deepNLU/mLearningNLU/embeddings/pretrained/GloVe/glove.840B.300d.bin\n"
     ]
    }
   ],
   "source": [
    "print(f'Loading static embeddings: {fname_embeddings}')\n",
    "embeddings = KeyedVectors.load_word2vec_format(fname_embeddings, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Classication\n",
    "\n",
    "- Integral component of NLU.\n",
    "- Broad range of applications:\n",
    "    - Chatbot\n",
    "    - Spam detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Chat Bot - Intent Classification\n",
    "- Software to conduct human-like conversation.\n",
    "- Intent classification only for our chatbot.\n",
    "- Why Python?\n",
    "    - Powerful computational frameworks (NumPy, Tensorflow, PyTorch)\n",
    "    - Ever-contributing community\n",
    "    - Open-source - Research friendly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1 Data Loading\n",
    "SNIPS dataset\n",
    "- 7 intents\n",
    "- 70 utterances for training set\n",
    "- 700 utterances for test set\n",
    "\n",
    "**pandas** library\n",
    "- ease of use\n",
    "- optimized for tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "./data/snips_train.csv\n",
      "./data/snips_test.csv\n",
      "\n",
      "GetWeather:\twhat do the cloud indicate in East Aurora\n",
      "BookRestaurant:\tI need a table at a restaurant serving carne pizzaiola for tamra davis, viola and dorothea\n",
      "AddToPlaylist:\tAdd artist Matt Noveskey to journey\n",
      "RateBook:\tRate the Under the Sign of Saturn 0 of 6\n",
      "PlayMusic:\tPlay me a top-ten song by Phil Ochs on Groove Shark\n",
      "SearchScreeningEvent:\tFind movie schedule for animated movies in the area\n",
      "SearchCreativeWork:\tPlease look up the television show, Noel Hill & Tony Linnane.\n",
      "\n",
      "Training set distribution:\n",
      "AddToPlaylist           10\n",
      "BookRestaurant          10\n",
      "GetWeather              10\n",
      "PlayMusic               10\n",
      "RateBook                10\n",
      "SearchCreativeWork      10\n",
      "SearchScreeningEvent    10\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Test set distribution:\n",
      "AddToPlaylist           100\n",
      "BookRestaurant          100\n",
      "GetWeather              100\n",
      "PlayMusic               100\n",
      "RateBook                100\n",
      "SearchCreativeWork      100\n",
      "SearchScreeningEvent    100\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Loading datasets...\\n{fname_snips_train}\\n{fname_snips_test}\\n')\n",
    "train_set = pd.read_csv(fname_snips_train, sep=';', header=None, names=['sample', 'class'])\n",
    "test_set = pd.read_csv(fname_snips_test, sep=';', header=None, names=['sample', 'class'])\n",
    "\n",
    "for the_class in train_set['class'].unique():\n",
    "    print(f\"{the_class}:\\t{train_set[train_set['class'] == the_class].iloc[0][0]}\")\n",
    "\n",
    "print(f\"\\nTraining set distribution:\\n{train_set['class'].value_counts().sort_index()}\\n\"\n",
    "      f\"\\nTest set distribution:\\n{test_set['class'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2 Text to vectors\n",
    "How to feed textual data into a machine learning model? &rightarrow; Transformation to numerical representations!\n",
    "\n",
    "**scikit-learn** bag-of-words:\n",
    "- For each word in the vocabulary &rightarrow; One dimension in vector space\n",
    "- Constant dimensionality in sentences\n",
    "\n",
    "A vector in ML:\n",
    "- Series of numbers\n",
    "- Each position &rightarrow; Intrinsic property of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what do the cloud indicate in East Aurora:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(train_set['sample'])\n",
    "train_set['vectors'] = train_set['sample'].apply(\n",
    "    lambda row: vectorizer.transform([row]).toarray()[0, :])\n",
    "test_set['vectors'] = test_set['sample'].apply(\n",
    "    lambda row: vectorizer.transform([row]).toarray()[0, :])\n",
    "\n",
    "print(f\"{train_set['sample'].iloc[0]}:\\n{train_set['vectors'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.3 Machine Learning\n",
    "**scikit-learn** logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Evaluating...\n",
      "Accuracy: 86.86%\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       AddToPlaylist       0.90      0.88      0.89       100\n",
      "      BookRestaurant       0.90      0.98      0.94       100\n",
      "          GetWeather       0.83      0.96      0.89       100\n",
      "           PlayMusic       0.97      0.91      0.94       100\n",
      "            RateBook       0.83      0.85      0.84       100\n",
      "  SearchCreativeWork       0.76      0.86      0.81       100\n",
      "SearchScreeningEvent       0.96      0.64      0.77       100\n",
      "\n",
      "            accuracy                           0.87       700\n",
      "           macro avg       0.88      0.87      0.87       700\n",
      "        weighted avg       0.88      0.87      0.87       700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression(solver='lbfgs', multi_class='auto', n_jobs=-1)\n",
    "\n",
    "print('Training...')\n",
    "logit.fit(X=train_set['vectors'].tolist(), y=train_set['class'].tolist())\n",
    "\n",
    "print('Evaluating...')\n",
    "predictions = logit.predict(X=test_set['vectors'].tolist())\n",
    "accuracy = accuracy_score(y_true=test_set['class'].tolist(), y_pred=predictions)\n",
    "report = classification_report(y_true=test_set['class'].tolist(), y_pred=predictions)\n",
    "print(f'Accuracy: {100 * accuracy:.2f}%\\n{report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How does our chatbot compare to the most popular intent detection tools?\n",
    "![SNIPS_comparison](https://i.ibb.co/3mhfHYF/SNIPS-comparison.png)\n",
    "\n",
    "Ready for a **harder NLU problem**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Spam detection\n",
    "**Quora** - Kaggle spam detection competition\n",
    "\n",
    "- Two (2) types of questions:\n",
    "    - sincere\n",
    "    - insincere\n",
    "- Difficult to distinguish with rule-based approach\n",
    "- Split original training set to training/test subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "./data/quora_train.csv\n",
      "./data/quora_test.csv\n",
      "\n",
      "0:\n",
      "How do I prepare for a software engineering job internship interview?\n",
      "\n",
      "1:\n",
      "Should China always walk three steps behind the US so that the US maintains its status in the world?\n",
      "\n",
      "\n",
      "Training set distribution:\n",
      "0    495\n",
      "1    505\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Test set distribution:\n",
      "0    512\n",
      "1    488\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Loading datasets...\\n{fname_quora_train}\\n{fname_quora_test}\\n')\n",
    "train_set = pd.read_csv(fname_quora_train, sep=';', header=None, names=['sample', 'class'])\n",
    "test_set = pd.read_csv(fname_quora_test, sep=';', header=None, names=['sample', 'class'])\n",
    "\n",
    "for the_class in train_set['class'].unique():\n",
    "    print(f\"{the_class}:\\n{train_set[train_set['class'] == the_class].iloc[10][0]}\\n\")\n",
    "\n",
    "print(f\"\\nTraining set distribution:\\n{train_set['class'].value_counts().sort_index()}\\n\"\n",
    "      f\"\\nTest set distribution:\\n{test_set['class'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2 Text to vectors\n",
    "**scikit-learn** bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.fit_transform(train_set['sample'])\n",
    "train_set['vectors'] = train_set['sample'].apply(\n",
    "    lambda row: vectorizer.transform([row]).toarray()[0, :])\n",
    "test_set['vectors'] = test_set['sample'].apply(\n",
    "    lambda row: vectorizer.transform([row]).toarray()[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3 Machine Learning\n",
    "**scikit-learn** logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Evaluating...\n",
      "Accuracy: 81.20%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82       512\n",
      "           1       0.83      0.77      0.80       488\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.81      0.81      0.81      1000\n",
      "weighted avg       0.81      0.81      0.81      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "logit.fit(X=train_set['vectors'].tolist(), y=train_set['class'].tolist())\n",
    "\n",
    "print('Evaluating...')\n",
    "predictions = logit.predict(X=test_set['vectors'].tolist())\n",
    "accuracy = accuracy_score(y_true=test_set['class'].tolist(), y_pred=predictions)\n",
    "report = classification_report(y_true=test_set['class'].tolist(), y_pred=predictions)\n",
    "print(f'Accuracy: {100 * accuracy:.2f}%\\n{report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "71% for the winning Kaggle Team &rightarrow; no direct comparison (different test set)\n",
    "\n",
    "But can we **do better**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. GloVe Embeddings\n",
    "\n",
    "- Statistics on large corpus &rightarrow; Word co-occurence matrix within a fixed window\n",
    "- Semantic similarity translates to proximity in the vector space\n",
    "- **gensim** library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.7401745319366455),\n",
       " ('guy', 0.7067893743515015),\n",
       " ('boy', 0.7045701742172241),\n",
       " ('he', 0.6831111907958984),\n",
       " ('men', 0.6729365587234497)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nearest neighbors\n",
    "embeddings.most_similar('man', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1 Text to vectors\n",
    "- Tokenize sentences\n",
    "- Assign embeddings to tokens\n",
    "- Sentence &rightarrow; **array of embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Assigning embeddings to tokens...\n",
      "How can Thanos be killed? -> ['how', 'can', 'thanos', 'be', 'killed', '?']\n",
      "[[-0.23205   0.47468  -0.38264  ...  0.33178   0.31545   0.37972 ]\n",
      " [-0.23857   0.35457  -0.30219  ... -0.35283   0.41888   0.13168 ]\n",
      " [ 0.27719  -0.08278   0.38441  ... -0.20608   0.24344  -0.12833 ]\n",
      " [-0.059177  0.10653  -0.21613  ... -0.42755   0.024956  0.02466 ]\n",
      " [-0.60482   0.37399   0.28446  ... -0.13601   0.30378  -0.084864]\n",
      " [-0.086864  0.19161   0.10915  ... -0.01516   0.11108   0.2065  ]]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenizing...')\n",
    "train_set['tokens'] = train_set['sample'].apply(lambda row: word_tokenize(row.lower()))\n",
    "test_set['tokens'] = test_set['sample'].apply(lambda row: word_tokenize(row.lower()))\n",
    "\n",
    "print('Assigning embeddings to tokens...')\n",
    "def token_to_vector(tokens: List[str], embeddings: KeyedVectors):\n",
    "    return np.array([embeddings[token] if token in embeddings.vocab\n",
    "                     else np.zeros(embeddings.vector_size) for token in tokens])\n",
    "\n",
    "train_set['token_vectors'] = train_set['tokens'].apply(lambda row: token_to_vector(row, embeddings))\n",
    "test_set['token_vectors'] = test_set['tokens'].apply(lambda row: token_to_vector(row, embeddings))\n",
    "\n",
    "print(f\"{train_set['sample'].iloc[0]} -> {train_set['tokens'].iloc[0]}\\n\"\n",
    "      f\"{train_set['token_vectors'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2 Sentence vectors\n",
    "- Sentences &rightarrow; **variable size** arrays\n",
    "- Average-pooling &rightarrow; Fixed-size vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence encoding...\n",
      "How can Thanos be killed?\n",
      "[-1.57381833e-01  2.36433327e-01 -2.04900000e-02  2.51661334e-02\n",
      " -9.79803782e-03 -2.41743326e-01  1.59131680e-02 -3.55949998e-02\n",
      " -1.11650735e-01  1.94314992e+00 -5.77013381e-02 -1.72157988e-01\n",
      "  4.69354987e-02  1.33230910e-01 -2.00982168e-01 -1.24464661e-01\n",
      " -3.41710038e-02  6.82393372e-01 -2.31642172e-01 -1.06247507e-01\n",
      "  5.78600056e-02 -1.63971677e-01  3.13919969e-02 -2.08065853e-01\n",
      " -5.97727261e-02  1.09268343e-02 -3.06570321e-01 -1.30958661e-01\n",
      "  1.37621328e-01 -5.04691713e-02 -2.55629331e-01 -1.11335002e-01\n",
      " -8.62485096e-02  4.54606600e-02 -1.57813337e-02  1.22769969e-02\n",
      "  1.18856668e-01  5.33842444e-02 -8.99899378e-02  7.60633424e-02\n",
      " -7.48885944e-02 -2.38888338e-01 -3.25999409e-03 -2.11189330e-01\n",
      "  7.31329992e-02  3.64403278e-02 -1.24055006e-01 -1.29284158e-01\n",
      " -1.09186850e-01  1.51953176e-01 -5.15733426e-03  3.20708267e-02\n",
      "  5.78449154e-03 -7.80333802e-02  3.40656415e-02  7.40098357e-02\n",
      " -1.38135150e-01 -3.84733342e-02  1.70907840e-01  1.09425165e-01\n",
      " -2.86607325e-01 -6.83411723e-03  8.40879977e-02  1.77961931e-01\n",
      "  1.39140993e-01  2.51999008e-03 -9.60560068e-02  8.78006592e-02\n",
      "  1.16798677e-01  8.41806754e-02  3.91323343e-02 -2.92221662e-02\n",
      "  1.98297665e-01 -5.18531762e-02  1.07297003e-01 -1.55103192e-01\n",
      "  2.09069982e-01 -2.65831858e-01  1.64825004e-02  1.51582241e-01\n",
      "  4.93280031e-02  9.93268490e-02 -1.56571001e-01  1.39531508e-01\n",
      "  4.59350646e-02 -1.28650069e-02  3.03148150e-01 -6.18767023e-01\n",
      "  4.26450074e-02 -8.21808279e-02 -8.95653367e-02 -4.92969044e-02\n",
      " -1.44309327e-01  3.80106643e-02 -8.80315006e-02 -8.82844925e-02\n",
      " -6.10868335e-02 -7.25909993e-02 -1.01857327e-01 -9.78766661e-03\n",
      " -1.44639507e-01  2.13068321e-01 -3.29069942e-02 -2.28996992e-01\n",
      "  2.46823337e-02 -6.84506714e-01 -1.02333330e-01  8.91475677e-02\n",
      " -7.59154186e-02  4.03483324e-02  5.86841702e-02 -3.21120173e-01\n",
      "  2.17605338e-01 -1.84435084e-01  1.47308007e-01  1.36071831e-01\n",
      "  5.73230051e-02  2.53979955e-02 -2.22553328e-01  2.82661375e-02\n",
      " -6.65945038e-02 -7.29004964e-02  2.33404830e-01 -9.47991684e-02\n",
      "  9.86170098e-02  3.21063325e-02 -9.25056711e-02 -2.88851678e-01\n",
      " -4.79993373e-02  6.56675771e-02 -2.30933335e-02 -3.88887036e-03\n",
      "  9.01548099e-03  1.41910672e-01  1.71041667e-01  9.39232763e-03\n",
      " -3.03846616e-02 -2.24506155e-01 -5.40156625e-02  2.09690090e-02\n",
      " -1.24905860e+00  7.91446641e-02  2.35221628e-02  2.32170403e-01\n",
      "  1.16708666e-01 -1.71089973e-02  4.96436656e-02  6.39529452e-02\n",
      " -1.18155323e-01 -1.99790999e-01  1.44257173e-01  4.64956723e-02\n",
      " -2.35583354e-02  4.99625020e-02 -5.04243262e-02 -1.51157990e-01\n",
      " -9.71138403e-02  1.26786651e-02 -4.97290008e-02  1.12884007e-01\n",
      "  6.80399984e-02 -1.53725341e-01 -2.18622327e-01  9.14633349e-02\n",
      "  2.24537492e-01 -5.28233238e-02 -4.37350087e-02 -7.79818371e-02\n",
      "  1.48233271e-03  1.84993193e-01  2.59424988e-02 -8.99355039e-02\n",
      "  2.79861659e-01 -2.91500986e-02 -1.17213823e-01 -6.37026653e-02\n",
      "  6.06868379e-02  9.09561738e-02 -2.48346683e-02 -7.83883408e-02\n",
      "  4.55660187e-02  1.36297166e-01 -1.24549441e-01 -1.85767666e-01\n",
      " -1.10983662e-01  3.11836656e-02 -3.87763344e-02 -1.24284970e-02\n",
      "  7.41754994e-02  7.99701586e-02  1.00847505e-01  2.30802000e-01\n",
      " -3.05528343e-01  1.14569165e-01  7.96149969e-02  1.92305982e-01\n",
      " -1.05435498e-01  1.19956806e-02  2.61711687e-01  1.72088325e-01\n",
      " -5.54138310e-02 -5.81423305e-02  7.66433328e-02  1.00771682e-02\n",
      "  2.20451653e-01  9.44233611e-02  6.49233311e-02  1.31630167e-01\n",
      "  1.61298320e-01 -1.43881842e-01 -2.88058352e-02 -1.46773025e-01\n",
      " -1.96408331e-01  6.61833286e-02  1.40845031e-02 -2.45461822e-01\n",
      "  9.83079895e-02  6.02979958e-02 -1.22706674e-01 -1.37402490e-01\n",
      "  5.54051585e-02  9.82768312e-02 -1.01285666e-01 -4.58303690e-02\n",
      " -7.85977468e-02  9.50333402e-02 -4.51065041e-02  2.15982005e-01\n",
      " -6.53478280e-02 -1.46504000e-01 -3.22770000e-01 -5.63035011e-02\n",
      "  2.04351664e-01  1.32849008e-01 -2.99475342e-01 -9.29761752e-02\n",
      "  1.23935334e-01 -9.39738378e-02 -1.39171660e-01  2.73385867e-02\n",
      "  1.69622838e-01  5.36546595e-02 -1.28060021e-02  4.65698354e-02\n",
      "  2.16476679e-01 -2.89101690e-01  1.22806668e-01 -8.27488378e-02\n",
      "  1.55616307e-03  1.10949948e-02  2.41456628e-02 -1.20090164e-01\n",
      " -5.03832614e-03  1.01751171e-01 -1.90227821e-01 -1.30693289e-02\n",
      " -1.14588670e-01 -3.22393291e-02  2.69338507e-02 -2.02885848e-02\n",
      "  1.19998336e-01  6.75373301e-02 -1.12078339e-02  1.24587625e-01\n",
      "  1.63781568e-02  9.12306681e-02 -6.93568364e-02  3.32463503e-01\n",
      "  2.18023345e-01  4.11918849e-01  9.15006623e-02 -1.93938371e-02\n",
      "  2.69898325e-02  2.47283299e-02 -1.85342833e-01  2.23176494e-01\n",
      " -1.29027173e-01  5.34331761e-02  1.51919767e-01  2.04374835e-01\n",
      "  2.02624992e-01  1.72294989e-01 -1.49036005e-01 -1.38694823e-01\n",
      "  1.95438638e-01  3.61573324e-03  1.93494156e-01 -1.34385988e-01\n",
      " -1.41660040e-02 -3.24609317e-02 -2.17578828e-01 -5.53113334e-02\n",
      " -1.44095644e-01 -1.01797663e-01  1.29322439e-01 -1.62429977e-02\n",
      " -2.68169999e-01 -1.34308338e-01  2.36264333e-01  8.82276669e-02]\n"
     ]
    }
   ],
   "source": [
    "print('Sentence encoding...')\n",
    "train_set['sentence_vectors'] = train_set['token_vectors'].apply(np.mean, axis=0)\n",
    "test_set['sentence_vectors'] = test_set['token_vectors'].apply(np.mean, axis=0)\n",
    "\n",
    "print(f\"{train_set['sample'].iloc[0]}\\n{train_set['sentence_vectors'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.3 Machine Learning\n",
    "**scikit-learn** logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Evaluating...\n",
      "Accuracy: 86.80%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87       512\n",
      "           1       0.87      0.85      0.86       488\n",
      "\n",
      "    accuracy                           0.87      1000\n",
      "   macro avg       0.87      0.87      0.87      1000\n",
      "weighted avg       0.87      0.87      0.87      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "logit.fit(X=train_set['sentence_vectors'].tolist(), y=train_set['class'].tolist())\n",
    "\n",
    "print('Evaluating...')\n",
    "predictions = logit.predict(X=test_set['sentence_vectors'].tolist())\n",
    "accuracy = accuracy_score(y_true=test_set['class'].tolist(), y_pred=predictions)\n",
    "report = classification_report(y_true=test_set['class'].tolist(), y_pred=predictions)\n",
    "print(f'Accuracy: {100 * accuracy:.2f}%\\n{report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Embeddings **>** bag-of-words\n",
    "\n",
    "*Note:* Both approaches do not account for word order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Transfer Learning\n",
    "- Deep learning requires huge amounts of data\n",
    "- Transfer Learning\n",
    "    - Deep learning model trained on enormous corpus\n",
    "    - State-of-the-art &rightarrow; **BERT** 340 million parameters\n",
    "    - HuggingFace **pytorch-transformers** library\n",
    "    - Task-specific classifier is attached to the large model\n",
    "    - Finetuning\n",
    "- Increased resources compared to statistical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training deep learning model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 01/20 | Loss: 0.5174 | LR: 4.75E-05: 100%|██████████| 125/125 [00:30<00:00,  4.14it/s]\n",
      "Epoch: 02/20 | Loss: 0.4667 | LR: 4.50E-05: 100%|██████████| 125/125 [00:29<00:00,  4.19it/s]\n",
      "Epoch: 03/20 | Loss: 0.3983 | LR: 4.25E-05: 100%|██████████| 125/125 [00:29<00:00,  4.22it/s]\n",
      "Epoch: 04/20 | Loss: 0.3539 | LR: 4.00E-05: 100%|██████████| 125/125 [00:29<00:00,  4.23it/s]\n",
      "Epoch: 05/20 | Loss: 0.3054 | LR: 3.75E-05: 100%|██████████| 125/125 [00:29<00:00,  4.24it/s]\n",
      "Epoch: 06/20 | Loss: 0.2729 | LR: 3.50E-05: 100%|██████████| 125/125 [00:29<00:00,  4.24it/s]\n",
      "Epoch: 07/20 | Loss: 0.2451 | LR: 3.25E-05: 100%|██████████| 125/125 [00:29<00:00,  4.25it/s]\n",
      "Epoch: 08/20 | Loss: 0.2199 | LR: 3.00E-05: 100%|██████████| 125/125 [00:29<00:00,  4.25it/s]\n",
      "Epoch: 09/20 | Loss: 0.2017 | LR: 2.75E-05: 100%|██████████| 125/125 [00:29<00:00,  4.24it/s]\n",
      "Epoch: 10/20 | Loss: 0.1857 | LR: 2.50E-05: 100%|██████████| 125/125 [00:29<00:00,  4.24it/s]\n",
      "Epoch: 11/20 | Loss: 0.1719 | LR: 2.25E-05: 100%|██████████| 125/125 [00:29<00:00,  4.25it/s]\n",
      "Epoch: 12/20 | Loss: 0.1599 | LR: 2.00E-05: 100%|██████████| 125/125 [00:29<00:00,  4.25it/s]\n",
      "Epoch: 13/20 | Loss: 0.1491 | LR: 1.75E-05: 100%|██████████| 125/125 [00:29<00:00,  4.25it/s]\n",
      "Epoch: 14/20 | Loss: 0.1401 | LR: 1.50E-05: 100%|██████████| 125/125 [00:29<00:00,  4.26it/s]\n",
      "Epoch: 15/20 | Loss: 0.1321 | LR: 1.25E-05: 100%|██████████| 125/125 [00:29<00:00,  4.26it/s]\n",
      "Epoch: 16/20 | Loss: 0.1251 | LR: 1.00E-05: 100%|██████████| 125/125 [00:29<00:00,  4.26it/s]\n",
      "Epoch: 17/20 | Loss: 0.1189 | LR: 7.50E-06: 100%|██████████| 125/125 [00:29<00:00,  4.26it/s]\n",
      "Epoch: 18/20 | Loss: 0.1134 | LR: 5.00E-06: 100%|██████████| 125/125 [00:29<00:00,  4.26it/s]\n",
      "Epoch: 19/20 | Loss: 0.1082 | LR: 2.50E-06: 100%|██████████| 125/125 [00:29<00:00,  4.28it/s]\n",
      "Epoch: 20/20 | Loss: 0.1034 | LR: 0.00E+00: 100%|██████████| 125/125 [00:29<00:00,  4.27it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating deep learning model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:06<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.90%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89       512\n",
      "           1       0.89      0.88      0.89       488\n",
      "\n",
      "    accuracy                           0.89      1000\n",
      "   macro avg       0.89      0.89      0.89      1000\n",
      "weighted avg       0.89      0.89      0.89      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training deep learning model...')\n",
    "deep_model = train(features=train_set['sample'], labels=train_set['class'], cache_dir=cache_dir,\n",
    "                   epochs=epochs)\n",
    "\n",
    "print('Evaluating deep learning model...')\n",
    "predictions = evaluate(model=deep_model, features=test_set['sample'], cache_dir=cache_dir)\n",
    "accuracy = accuracy_score(y_true=test_set['class'].tolist(), y_pred=predictions)\n",
    "report = classification_report(y_true=test_set['class'].tolist(), y_pred=predictions)\n",
    "print(f'Accuracy: {100 * accuracy:.2f}%\\n{report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "- Started with nothing (bag-of-words)\n",
    "- Enriched information via word embeddings\n",
    "- Finetuning of BERT to a downstream task provided us with state-of-the-art results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
